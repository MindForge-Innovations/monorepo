{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD API KEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load API Specifications Clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from openapi_spec_validator import validate\n",
    "\n",
    "\n",
    "class Endpoint:\n",
    "    def __init__(self, api_title: str, api_description: str, api_version: str, openapi_version: str, path: str, method: str, details: Dict[str, Any]):\n",
    "        self.api_title = api_title\n",
    "        self.api_description = api_description\n",
    "        self.api_version = api_version\n",
    "        self.openapi_version = openapi_version\n",
    "        self.path = path\n",
    "        self.method = method.upper()\n",
    "        self.details = details\n",
    "    \n",
    "    def generate_description(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Génère une description textuelle pour l'endpoint.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            La description textuelle de l'endpoint sous forme de dictionnaire.\n",
    "        \"\"\"\n",
    "        endpoint_description = self.details.get('summary', '-')\n",
    "        parameters = {param['name']: param for param in self.details.get('parameters', [])}\n",
    "        tags = self.details.get('tags', [])\n",
    "        responses = self.details.get('responses', {})\n",
    "\n",
    "        infos = {\n",
    "            \"API_Title\": self.api_title,\n",
    "            \"API_Description\": self.api_description,\n",
    "            \"API_Version\": self.api_version,\n",
    "            \"OpenAPI_Version\": self.openapi_version,\n",
    "            \"Route\": self.path,\n",
    "            \"Method\": self.method,\n",
    "            \"Endpoint_Description\": endpoint_description,\n",
    "            \"Parameters\": parameters,\n",
    "            \"Tags\": tags,\n",
    "            \"Responses\": responses\n",
    "        }\n",
    "\n",
    "        return json.dumps(infos)\n",
    "\n",
    "class API:\n",
    "    def __init__(self, api_data: Dict[str, Any]):\n",
    "        self.api_data = api_data\n",
    "        self.title = api_data.get('info', {}).get('title', '-')\n",
    "        self.description = api_data.get('info', {}).get('description', '-').replace('\\n', ' ')\n",
    "        self.version = api_data.get('info', {}).get('version', '-')\n",
    "        self.openapi_version = api_data.get('openapi', '-')\n",
    "        self.endpoints = []\n",
    "        self._validate_spec()\n",
    "        self.endpoints = self._create_endpoints()\n",
    "\n",
    "    def _validate_spec(self):\n",
    "        \"\"\"\n",
    "        Valide la spécification OpenAPI de l'API.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        OpenAPIValidationError\n",
    "            Si la spécification OpenAPI n'est pas valide.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            validate(self.api_data)\n",
    "        except Exception as e:\n",
    "            raise SyntaxError(f\"Invalid OpenAPI specification: {e}\") from e\n",
    "    \n",
    "    def _create_endpoints(self) -> List[Endpoint]:\n",
    "        endpoints = []\n",
    "        for path, methods in self.api_data.get('paths', {}).items():\n",
    "            for method, details in methods.items():\n",
    "                endpoints.append(Endpoint(self.title, self.description, self.version, self.openapi_version, path, method, details))\n",
    "        return endpoints\n",
    "    \n",
    "    def generate_endpoints_descriptions(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Génère des descriptions pour tous les endpoints de l'API.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Dict[str, Any]]\n",
    "            Une liste de dictionnaires, chacun contenant la description d'un endpoint.\n",
    "        \"\"\"\n",
    "        return [endpoint.generate_description() for endpoint in self.endpoints]\n",
    "\n",
    "def load_api_specs(file_path: Path) -> List[API]:\n",
    "    \"\"\"\n",
    "    Charge les données des APIs à partir d'un fichier JSON et crée des instances d'API.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Path\n",
    "        Le chemin du fichier contenant les données de l'API au format JSON.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[API]\n",
    "        Une liste d'instances d'API.\n",
    "    \"\"\"\n",
    "    with file_path.open('r') as file:\n",
    "        api_specs = json.load(file)\n",
    "        return [API(api_data) for api_data in api_specs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/embeddings.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     joblib\u001b[38;5;241m.\u001b[39mdump(spec_embeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/embeddings.joblib\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     spec_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/embeddings.joblib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(spec_embeddings)\n",
      "File \u001b[0;32m~/anaconda3/envs/langchain/lib/python3.11/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/embeddings.joblib'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from openai import OpenAI\n",
    "import joblib\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "API_DOC_PATH = Path(\"../data/APIdocumentation.json\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "COMPUTE_EMBEDDINGS = False\n",
    "\n",
    "if COMPUTE_EMBEDDINGS:\n",
    "    # Chargement des APIs\n",
    "    apis = load_api_specs(API_DOC_PATH)\n",
    "\n",
    "    spec_embeddings = []\n",
    "    for api in tqdm(apis):\n",
    "        for endpoint in api.endpoints:\n",
    "            endpoint_json = endpoint.generate_description()\n",
    "            embedding = get_embedding(endpoint_json)\n",
    "            spec_embeddings.append(\n",
    "                {\n",
    "                    \"embeddings\": embedding,\n",
    "                    \"documents\": endpoint_json,\n",
    "                    \"metadatas\": {\"title\":api.title, \"version\":api.version, \"openapi\":api.openapi_version, \"endpoint\":endpoint.path, \"http_method\":endpoint.method},\n",
    "                    \"id\": uuid.uuid4()\n",
    "                }\n",
    "            )\n",
    "    joblib.dump(spec_embeddings, \"../data/embeddings.joblib\")\n",
    "else:\n",
    "    spec_embeddings = joblib.load(\"../data/embeddings.joblib\")\n",
    "    df = pd.DataFrame(spec_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"api_endpoints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m----> 2\u001b[0m     documents\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m      3\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m      4\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mmetadatas\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m      5\u001b[0m     ids\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mid\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=df.documents.tolist(),\n",
    "    embeddings=df.embeddings.tolist(),\n",
    "    metadatas=df.metadatas.tolist(),\n",
    "    ids=df.id.apply(str).to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"I want to blur my face\"\n",
    "\n",
    "REQUEST = True\n",
    "if REQUEST:\n",
    "    embedding = get_embedding(request)\n",
    "    joblib.dump(embedding, \"../data/request_embedding.joblib\")\n",
    "else:\n",
    "    embedding = joblib.load(\"../data/request_embedding.joblib\")\n",
    "\n",
    "results = collection.query(query_embeddings = embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['distances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spec_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspec_embeddings\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spec_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "spec_embeddings[0]['documents']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting to the real cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    host=\"192.168.49.2\",\n",
    "    port=31791,\n",
    "    headers={\"Authorization\": \"Bearer Hnq6GBxJlHfJEGoxrazxXdoG3aQ0gncP\"}\n",
    ")\n",
    "collection = chroma_client.get_or_create_collection(name=\"api_endpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"I want to blur my face\"\n",
    "\n",
    "REQUEST = True\n",
    "if REQUEST:\n",
    "    embedding = get_embedding(request)\n",
    "    joblib.dump(embedding, \"../data/request_embedding.joblib\")\n",
    "else:\n",
    "    embedding = joblib.load(\"../data/request_embedding.joblib\")\n",
    "\n",
    "results = collection.query(query_embeddings = embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['4ff67daa-67e2-4e30-be63-1ea9f5cf33cb',\n",
       "   'f7a4ce4c-61f0-497d-9e90-0ed1d2229d03',\n",
       "   '627355dc-b178-4e51-8c06-fbe5dfbb3d35',\n",
       "   'ce1596f6-dc38-4ee3-a535-828fd09aec84',\n",
       "   'db046546-fab2-4fbc-b602-4236d89ebeb3',\n",
       "   'c83b0b3a-8c9e-45e3-8201-60c1784aac13',\n",
       "   '7c5b051d-10aa-4d04-ac82-69a93e1f20d0',\n",
       "   '4aa2bc9e-050a-4832-b66f-0d7d0ef03d04',\n",
       "   '8be404e3-a700-4d31-8d64-a855d6b7f35d',\n",
       "   '348c4971-4f9d-4352-8ceb-e7f15ee5dd0a']],\n",
       " 'distances': [[1.1214076595255955,\n",
       "   1.2169302115586516,\n",
       "   1.2244906658203774,\n",
       "   1.4515220689875672,\n",
       "   1.4929895041890096,\n",
       "   1.5089628713579588,\n",
       "   1.5180095978740673,\n",
       "   1.5909513525745822,\n",
       "   1.5967385579611666,\n",
       "   1.6241667619100735]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'endpoint': '/compute',\n",
       "    'http_method': 'POST',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Image Blur API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Image Blur API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/tasks/{task_id}/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Image Blur API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/compute',\n",
       "    'http_method': 'POST',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Analyzer API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Analyzer API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/compute',\n",
       "    'http_method': 'POST',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Detection API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/tasks/{task_id}/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Analyzer API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/tasks/{task_id}/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Detection API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/status',\n",
       "    'http_method': 'GET',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Face Detection API.',\n",
       "    'version': '1.0.0'},\n",
       "   {'endpoint': '/compute',\n",
       "    'http_method': 'POST',\n",
       "    'openapi': '3.1.0',\n",
       "    'title': 'Digit Recognition API.',\n",
       "    'version': '1.0.0'}]],\n",
       " 'documents': [['{\"API_Title\": \"Image Blur API.\", \"API_Description\": \" This service blurs the image in the given areas. The areas are given as a list of [x1, y1, x2, y2] coordinates. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/compute\", \"Method\": \"POST\", \"Endpoint_Description\": \"Compute task\", \"Parameters\": {}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Image Blur API.\", \"API_Description\": \" This service blurs the image in the given areas. The areas are given as a list of [x1, y1, x2, y2] coordinates. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get service availability\", \"Parameters\": {}, \"Tags\": [\"Service\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}}}',\n",
       "   '{\"API_Title\": \"Image Blur API.\", \"API_Description\": \" This service blurs the image in the given areas. The areas are given as a list of [x1, y1, x2, y2] coordinates. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/tasks/{task_id}/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get task status\", \"Parameters\": {\"task_id\": {\"name\": \"task_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Task Id\"}}}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Face Analyzer API.\", \"API_Description\": \" Analyse faces in images. Returns a JSON object with the following fields: - age (Age of the person in the image), - region (Region of the person in the image), - gender (Gender of the person in the image), - race (Race of the person in the image), - dominant_race (Dominant race of the person in the image), - emotion (Emotion of the person in the image), - dominant_emotion (The dominant emotion of the person in the image) \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/compute\", \"Method\": \"POST\", \"Endpoint_Description\": \"Compute task\", \"Parameters\": {}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Face Analyzer API.\", \"API_Description\": \" Analyse faces in images. Returns a JSON object with the following fields: - age (Age of the person in the image), - region (Region of the person in the image), - gender (Gender of the person in the image), - race (Race of the person in the image), - dominant_race (Dominant race of the person in the image), - emotion (Emotion of the person in the image), - dominant_emotion (The dominant emotion of the person in the image) \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get service availability\", \"Parameters\": {}, \"Tags\": [\"Service\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}}}',\n",
       "   '{\"API_Title\": \"Face Detection API.\", \"API_Description\": \" This service detects faces in images and returns the coordinates of the bounding boxes. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/compute\", \"Method\": \"POST\", \"Endpoint_Description\": \"Compute task\", \"Parameters\": {}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Face Analyzer API.\", \"API_Description\": \" Analyse faces in images. Returns a JSON object with the following fields: - age (Age of the person in the image), - region (Region of the person in the image), - gender (Gender of the person in the image), - race (Race of the person in the image), - dominant_race (Dominant race of the person in the image), - emotion (Emotion of the person in the image), - dominant_emotion (The dominant emotion of the person in the image) \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/tasks/{task_id}/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get task status\", \"Parameters\": {\"task_id\": {\"name\": \"task_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Task Id\"}}}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Face Detection API.\", \"API_Description\": \" This service detects faces in images and returns the coordinates of the bounding boxes. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/tasks/{task_id}/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get task status\", \"Parameters\": {\"task_id\": {\"name\": \"task_id\", \"in\": \"path\", \"required\": true, \"schema\": {\"type\": \"string\", \"format\": \"uuid\", \"title\": \"Task Id\"}}}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}',\n",
       "   '{\"API_Title\": \"Face Detection API.\", \"API_Description\": \" This service detects faces in images and returns the coordinates of the bounding boxes. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/status\", \"Method\": \"GET\", \"Endpoint_Description\": \"Get service availability\", \"Parameters\": {}, \"Tags\": [\"Service\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}}}',\n",
       "   '{\"API_Title\": \"Digit Recognition API.\", \"API_Description\": \" Recognizes a digit in an image using mnist trained model. \", \"API_Version\": \"1.0.0\", \"OpenAPI_Version\": \"3.1.0\", \"Route\": \"/compute\", \"Method\": \"POST\", \"Endpoint_Description\": \"Compute task\", \"Parameters\": {}, \"Tags\": [\"Tasks\"], \"Responses\": {\"200\": {\"description\": \"Successful Response\", \"content\": {\"application/json\": {\"schema\": {}}}}, \"422\": {\"description\": \"Validation Error\", \"content\": {\"application/json\": {\"schema\": {\"$ref\": \"#/components/schemas/HTTPValidationError\"}}}}}}']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
